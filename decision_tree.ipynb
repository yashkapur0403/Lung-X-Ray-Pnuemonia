{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNixKX1YjTKXYocDkRTAyM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashkapur0403/Neural-Networks-Practise/blob/main/decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: Mount Drive and Extract Dataset\n",
        "from google.colab import drive\n",
        "import zipfile, os, cv2\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')  # allow permission\n",
        "zip_path = \"/content/drive/My Drive/Colab Notebooks/chest_xray.zip\"\n",
        "extract_path = \"/content/chest_xray\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# STEP 2: Load images and prepare data\n",
        "def load_images_from_folder(folder_path, label):\n",
        "    data = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        path = os.path.join(folder_path, filename)\n",
        "        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (64, 64))\n",
        "            img_flat = img.flatten() / 255.0\n",
        "            data.append((img_flat, label))\n",
        "    return data\n",
        "\n",
        "normal_data = load_images_from_folder(\"/content/chest_xray/chest_xray/train/NORMAL\", 0)\n",
        "pneumonia_data = load_images_from_folder(\"/content/chest_xray/chest_xray/train/PNEUMONIA\", 1)\n",
        "all_data = normal_data + pneumonia_data\n",
        "np.random.shuffle(all_data)\n",
        "\n",
        "X_train = np.array([x for x, _ in all_data])\n",
        "y_train = np.array([y for _, y in all_data])\n",
        "\n",
        "normal_test = load_images_from_folder(\"/content/chest_xray/chest_xray/test/NORMAL\", 0)\n",
        "pneumonia_test = load_images_from_folder(\"/content/chest_xray/chest_xray/test/PNEUMONIA\", 1)\n",
        "all_test = normal_test + pneumonia_test\n",
        "np.random.shuffle(all_test)\n",
        "\n",
        "X_test = np.array([x for x, _ in all_test])\n",
        "y_test = np.array([y for _, y in all_test])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hbW6q9t9ONc",
        "outputId": "0f507b6e-d267-47bb-c1fc-a13b36a00584"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvb6UUSl83tH",
        "outputId": "d8f8f666-1d64-4179-f488-37cf10837a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 91.0 %\n",
            "Test Accuracy: 64.90384615384616 %\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gini Impurity\n",
        "def gini(y):\n",
        "    classes, counts = np.unique(y, return_counts=True)     # return_counts is a np function # this line gives us 2 variables....ek me kitni classes [0,1] next me count\n",
        "    probs = counts / counts.sum()\n",
        "    return 1 - np.sum(probs ** 2)                   # gini probability formula\n",
        "\n",
        "# Splitting the dataset\n",
        "def split(X, y, feature_index, threshold):\n",
        "    left_mask = X[:, feature_index] <= threshold         # less than threshold then left otherwise right leaf\n",
        "    right_mask = ~left_mask                            # mask is not a np fxn...its just a notation generally used to represent boolean arrays in python\n",
        "    return X[left_mask], y[left_mask], X[right_mask], y[right_mask]\n",
        "\n",
        "# Best split - EFFICIENT VERSION\n",
        "def best_split(X, y):\n",
        "    m, n = X.shape\n",
        "    best_gini = 1\n",
        "    best_idx, best_thresh = None, None\n",
        "\n",
        "    for feature_index in range(n):\n",
        "        feature_values = X[:, feature_index]\n",
        "        min_val, max_val = feature_values.min(), feature_values.max()\n",
        "\n",
        "        # Use fewer thresholds but better distributed - this prevents infinite loops\n",
        "        if min_val == max_val:\n",
        "            continue\n",
        "\n",
        "        # Sample 15 thresholds between min and max (compromise between your 10 and efficiency)\n",
        "        thresholds = np.linspace(min_val, max_val, 15)[1:-1]  # exclude min and max\n",
        "\n",
        "        for t in thresholds:\n",
        "            _, y_left, _, y_right = split(X, y, feature_index, t)\n",
        "            if len(y_left) == 0 or len(y_right) == 0:     # if empty then ignore\n",
        "                continue\n",
        "\n",
        "            # Add minimum samples requirement to prevent overfitting - STRICTER\n",
        "            if len(y_left) < 15 or len(y_right) < 15:\n",
        "                continue\n",
        "\n",
        "            g = (len(y_left) * gini(y_left) + len(y_right) * gini(y_right)) / m       # total gini\n",
        "            if g < best_gini:                         # if total calc gini less than current... we repalce and update with the current one\n",
        "                best_gini = g\n",
        "                best_idx = feature_index               # jis feature id pe mili gini, usko best idx me update karte jayenge\n",
        "                best_thresh = t                        # similarly for the threshold part too\n",
        "    return best_idx, best_thresh\n",
        "\n",
        "# Build the tree\n",
        "class Node:\n",
        "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, value=None):\n",
        "        self.feature_index = feature_index\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.value = value  # used for leaf node\n",
        "\n",
        "def build_tree(X, y, depth=0, max_depth=6, min_samples_split=25):  # Much more restrictive parameters\n",
        "    # Add more stopping conditions to prevent overfitting\n",
        "    if (len(set(y)) == 1 or\n",
        "        depth == max_depth or\n",
        "        len(y) < min_samples_split):\n",
        "        # Leaf node\n",
        "        values, counts = np.unique(y, return_counts=True)\n",
        "        return Node(value=values[np.argmax(counts)])\n",
        "\n",
        "    feat_idx, thresh = best_split(X, y)\n",
        "    if feat_idx is None:\n",
        "        values, counts = np.unique(y, return_counts=True)\n",
        "        return Node(value=values[np.argmax(counts)])\n",
        "\n",
        "    X_left, y_left, X_right, y_right = split(X, y, feat_idx, thresh)\n",
        "\n",
        "    # Ensure minimum samples in each split - STRICTER\n",
        "    if len(y_left) < 15 or len(y_right) < 15:\n",
        "        values, counts = np.unique(y, return_counts=True)\n",
        "        return Node(value=values[np.argmax(counts)])\n",
        "\n",
        "    left_child = build_tree(X_left, y_left, depth + 1, max_depth, min_samples_split)\n",
        "    right_child = build_tree(X_right, y_right, depth + 1, max_depth, min_samples_split)\n",
        "    return Node(feature_index=feat_idx, threshold=thresh, left=left_child, right=right_child)\n",
        "\n",
        "# Predict\n",
        "def predict_one(x, node):\n",
        "    if node.value is not None:\n",
        "        return node.value\n",
        "    if x[node.feature_index] <= node.threshold:\n",
        "        return predict_one(x, node.left)\n",
        "    else:\n",
        "        return predict_one(x, node.right)\n",
        "\n",
        "def predict(X, tree):\n",
        "    return np.array([predict_one(row, tree) for row in X])\n",
        "\n",
        "# Shuffle data manually without sklearn\n",
        "def manual_shuffle(X, y, random_state=42):\n",
        "    np.random.seed(random_state)\n",
        "    indices = np.random.permutation(len(X))\n",
        "    return X[indices], y[indices]\n",
        "\n",
        "# Use manual shuffle instead of sklearn - USE SMALLER SAMPLE FOR FASTER EXECUTION\n",
        "X_shuffled, y_shuffled = manual_shuffle(X_train, y_train, random_state=42)\n",
        "X_small = X_shuffled[:100]  # Reduced from 500 to 100 for faster execution\n",
        "y_small = y_shuffled[:100]\n",
        "\n",
        "# Build tree with better parameters\n",
        "tree = build_tree(X_small, y_small, max_depth=8, min_samples_split=15)\n",
        "preds = predict(X_small, tree)\n",
        "\n",
        "accuracy = np.sum(preds == y_small) / len(y_small)\n",
        "print(\"Training Accuracy:\", accuracy * 100 , \"%\")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_preds = predict(X_test, tree)\n",
        "test_acc = np.sum(test_preds == y_test) / len(y_test)\n",
        "print(\"Test Accuracy:\", test_acc * 100, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(\"Confusion Matrix on Training Data:\")\n",
        "print(confusion_matrix(y_small, preds))\n"
      ],
      "metadata": {
        "id": "vDGtqYblRMay"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}